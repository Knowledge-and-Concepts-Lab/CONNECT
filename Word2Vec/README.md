# Word2Vec

Collect resources for understanding word2vec here. Below are links to materials for discussion on 6/9:

**Lighter reading**
Blog post: https://jalammar.github.io/illustrated-word2vec/

For visual learners, there is this Youtube video (17 minutes): https://www.youtube.com/watch?v=QyrUentbkvw&ab_channel=JordanBoyd-Graber

PyTorch implementation of the continuous-bag-of-words (CBOW) approach to fitting models here: https://rguigoures.github.io/word2vec_pytorch/

**Reminder:** The goal for Tuesday will be to just make sure we all understand what word2vec is meant to do and why it is interesting, as well as getting an initial sense of the steps involved in fitting such a model to some corpus of data. We will then spend a week playing around with getting it to work on something of interest. 

**Please add links to other resources you find here:**

**More detailed descriptions of the model:**

Chapter 6 of Jurafsy and Martin's NLP textbook is a helpful detailed guide to some of the concepts related to Word2Vec and embeddings in general. Specifically pages 18-22 are great primers on the SGD-based training process for word2vec - https://web.stanford.edu/~jurafsky/slp3/6.pdf

A couple of more detailed blog posts that disassemble the model and some of the reasoning behind various elements/steps:
https://aegis4048.github.io/demystifying_neural_network_in_skip_gram_language_modeling#eq-18

https://aegis4048.github.io/demystifying_neural_network_in_skip_gram_language_modeling#eq-18
